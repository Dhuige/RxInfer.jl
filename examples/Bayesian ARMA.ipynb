{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "blessed-brake",
   "metadata": {},
   "source": [
    "# Bayesian ARMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate local environment, see `Project.toml`\n",
    "import Pkg; Pkg.activate(\".\"); Pkg.instantiate(); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-million",
   "metadata": {},
   "source": [
    "This notebook shows how Bayesian [ARMA model](https://en.wikipedia.org/wiki/Autoregressive%E2%80%93moving-average_model#Applications) can be implemeted in **RxInfer.jl**. For theoretical details on Varitional Inference for ARMA model, we refer the reader to the following [paper](https://ieeexplore.ieee.org/document/7798432). The Bayesian ARMA model can be written as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-advice",
   "metadata": {},
   "source": [
    "\n",
    "$$\\begin{aligned}\n",
    "e_t \\sim \\mathcal{N}(0, \\gamma^{-1}) \\quad\n",
    "\\theta &\\sim \\mathcal{MN}(\\mathbf{0}, \\mathbf{I}) \\quad\n",
    "\\eta \\sim \\mathcal{MN}(\\mathbf{0}, \\mathbf{I}) \\\\\n",
    "\\mathbf{h}_0 &\\sim \\mathcal{MN}\\left(\\begin{bmatrix}\n",
    "e_{-1} \\\\\n",
    "e_{-2}\n",
    "\\end{bmatrix}, \\mathbf{I}\\right) \\\\\n",
    "\\mathbf{h}_t &= \\mathbf{S}\\mathbf{h}_{t-1} + \\mathbf{c} e_{t-1} \\\\\n",
    "\\mathbf{x}_t &= \\boldsymbol{\\theta}^\\top\\mathbf{x}_{t-1} + \\boldsymbol{\\eta}^\\top\\mathbf{h}_{t} + e_t \n",
    "\\end{aligned}$$\n",
    "\n",
    "where shift matrix $\\mathbf{S}$ is\n",
    "$$\\begin{aligned}\\mathbf{S} = \\begin{pmatrix}\n",
    "0 & 0 \\\\\n",
    "1 & 0 \n",
    "\\end{pmatrix}\\end{aligned}$$\n",
    "and unit vector $\\mathbf{c}$: $$\\begin{aligned}\\mathbf{c}=[1, 0]\\end{aligned}$$ when MA order is $2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-registrar",
   "metadata": {},
   "source": [
    "In this way, $\\mathbf{h}_t$ containing errors $e_t$ can be viewed as hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-hospital",
   "metadata": {},
   "source": [
    "In short, the Bayesian ARMA model has two intractabilities: (1) induced by the multiplication of two Gaussian RVs, i.e., $\\boldsymbol{\\eta}^\\top\\mathbf{h}_{t}$, (2) induced by errors $e_t$ that prevents analytical update of precision parameter $\\gamma$ (this can be easily seen when constructing the Factor Graph, i.e. there is a loop). Both problems can be easily resolved in **RxInfer.jl**, by creating a hybrid inference algorithm based on Loopy Variational Message Passing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "using RxInfer, LinearAlgebra, CSV, DataFrames, Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-cross",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define shift function\n",
    "function shift(dim)\n",
    "    S = Matrix{Float64}(I, dim, dim)\n",
    "    for i in dim:-1:2\n",
    "           S[i,:] = S[i-1, :]\n",
    "    end\n",
    "    S[1, :] = zeros(dim)\n",
    "    return S\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function ARMA(n, x_prev, h_prior, γ_prior, τ_prior, η_prior, θ_prior, p, q)\n",
    "        \n",
    "    c = zeros(p); c[1] = 1.0; # AR\n",
    "    b = zeros(q); b[1] = 1.0; # MA\n",
    "    S = shift(q); # MA\n",
    "    \n",
    "    # initialize variables\n",
    "    h  = randomvar(n-1)\n",
    "    e  = randomvar(n)\n",
    "    z  = randomvar(n)\n",
    "    \n",
    "    x  = datavar(Float64, n) where { allow_missing = true }\n",
    "    \n",
    "    # priors\n",
    "    γ  ~ Gamma(shape = shape(γ_prior), rate = rate(γ_prior))\n",
    "    η  ~ MvNormal(mean = mean(η_prior), precision = precision(η_prior))\n",
    "    θ  ~ MvNormal(mean = mean(θ_prior), precision = precision(θ_prior))\n",
    "    τ  ~ Gamma(shape = shape(τ_prior), rate = rate(τ_prior))\n",
    "    \n",
    "    # initial\n",
    "    h_0  ~ MvNormal(mean = mean(h_prior), precision = precision(h_prior))\n",
    "    z[1] ~ AR(h_0, η, τ) \n",
    "    e[1] ~ Normal(mean = 0.0, precision = γ)\n",
    "\n",
    "    x[1] ~ dot(b, z[1]) + dot(θ, x_prev[1]) + e[1]\n",
    "    \n",
    "    h_prev = h_0\n",
    "    for t in 1:n-1\n",
    "        \n",
    "        e[t+1] ~ Normal(mean = 0.0, precision = γ)\n",
    "        h[t] ~ S*h_prev + b*e[t]\n",
    "        z[t+1] ~ AR(h[t], η, τ)\n",
    "        x[t+1] ~ dot(z[t+1], b) + dot(θ, x_prev[t]) + e[t+1]\n",
    "        h_prev = h[t]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-opinion",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-elder",
   "metadata": {},
   "source": [
    "To validate our model and inference, we will use the S&P 500 stock data from [Kaggle](https://www.kaggle.com/code/purvasingh/time-series-analysis-with-arma-and-arima/data?select=all_stocks_5yr.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-mainstream",
   "metadata": {},
   "source": [
    "Once the dataset is downloaded, we can load it into DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = CSV.read(\"data/all_stocks_5yr.csv\", DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use \"close\" column\n",
    "# for the sake of example, we will use only 200 entries out of 619040\n",
    "x_data = filter(!ismissing, x_df[:, 5])[1:200];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "plot(x_data, xlabel=\"day\", ylabel=\"price\", label=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_order = 10 # AR\n",
    "q_order = 4 # MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "train_size = 100\n",
    "x_prev_train = [Float64.(x_data[i+p_order-1:-1:i]) for i in 1:length(x_data)-p_order][1:train_size]\n",
    "x_train = Float64.(x_data[p_order+1:end])[1:train_size];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-sessions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "x_prev_test = [Float64.(x_data[i+p_order-1:-1:i]) for i in 1:length(x_data)-p_order][train_size+1:end]\n",
    "x_test = Float64.(x_data[p_order+1:end])[train_size+1:end];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-positive",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraints are needed for performing VMP\n",
    "arma_constraints = @constraints begin\n",
    "    q(z, h_0, h, η, τ, γ,e) = q(h_0)q(z, h)q(η)q(τ)q(γ)q(e)\n",
    "end;\n",
    "\n",
    "@meta function ARMAmeta(q)\n",
    "    AR() -> ARMeta(Multivariate, q, ARsafe())\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_prior = MvNormalMeanPrecision(zeros(q_order), diageye(q_order))\n",
    "γ_prior = GammaShapeRate(1e4, 1.0)\n",
    "τ_prior = GammaShapeRate(1e2, 1.0)\n",
    "η_prior = MvNormalMeanPrecision(zeros(q_order), diageye(q_order))\n",
    "θ_prior = MvNormalMeanPrecision(zeros(p_order), diageye(p_order));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "arma_imarginals = (h_0 = h_prior, h = h_prior, γ = γ_prior, τ = τ_prior, η = η_prior, θ = θ_prior);\n",
    "arma_imessages  = (h_0 = h_prior, h = h_prior);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First execution is slow due to Julia's init compilation \n",
    "result = inference(\n",
    "    model = ARMA(length(x_train), x_prev_train, h_prior, γ_prior, τ_prior, η_prior, θ_prior, p_order, q_order), \n",
    "    meta = ARMAmeta(q_order),\n",
    "    data  = (x = x_train, ),\n",
    "    free_energy = false, # we are not interested in the free energy in this experiment\n",
    "    initmarginals = arma_imarginals,\n",
    "    initmessages  = arma_imessages,\n",
    "    constraints   = arma_constraints,\n",
    "    iterations    = 50,\n",
    "    options       = (limit_stack_depth = 100, ),\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(mean.(result.posteriors[:e][end]), ribbon=std.(result.posteriors[:e][end]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract posteriors\n",
    "h_posterior = result.posteriors[:h][end][end]\n",
    "γ_posterior = result.posteriors[:γ][end]\n",
    "τ_posterior = result.posteriors[:τ][end]\n",
    "η_posterior = result.posteriors[:η][end]\n",
    "θ_posterior = result.posteriors[:θ][end];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prediction function is aimed at approximating the predictive posterior distribution\n",
    "# It triggers the rules in the generative order (in future, RxInfer.jl will provide this function out of the box)\n",
    "function prediction(x_prev, h_posterior, γ_posterior, τ_posterior, η_posterior, θ_posterior, p, q)\n",
    "    h_out = MvNormalMeanPrecision(mean(h_posterior), precision(h_posterior))\n",
    "    ar_out = @call_rule AR(:y, Marginalisation) (m_x=h_out, q_θ=η_posterior, q_γ=τ_posterior, meta=ARMeta(Multivariate, p, ARsafe()))\n",
    "    c = zeros(p); c[1] = 1.0\n",
    "    b = zeros(q); b[1] = 1.0\n",
    "    ar_dot_out = @call_rule typeof(dot)(:out, Marginalisation) (m_in1=PointMass(b), m_in2=ar_out, meta=ReactiveMP.TinyCorrection())\n",
    "    θ_out = MvNormalMeanPrecision(mean(θ_posterior), precision(θ_posterior))\n",
    "    ma_dot_out = @call_rule typeof(dot)(:out, Marginalisation) (m_in1=PointMass(x_prev), m_in2=θ_out, meta=ReactiveMP.TinyCorrection())\n",
    "    e_out = @call_rule NormalMeanPrecision(:out, Marginalisation) (q_μ=PointMass(0.0), q_τ=mean(γ_posterior))\n",
    "    ar_ma = @call_rule typeof(+)(:out, Marginalisation) (m_in1=ar_dot_out, m_in2=ma_dot_out)  \n",
    "    @call_rule typeof(+)(:out, Marginalisation) (m_in1=ar_ma, m_in2=e_out)  \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for x_prev in x_prev_test\n",
    "    push!(predictions, prediction(x_prev, h_posterior, γ_posterior, τ_posterior, η_posterior, θ_posterior, p_order, q_order))\n",
    "    # after every new prediction we can actually \"retrain\" the model to use the power of Bayesian approach\n",
    "    # we will skip this part for now\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x_test, label=\"test data\", legend=:topleft)\n",
    "plot!(mean.(predictions)[1:end], ribbon=std.(predictions)[1:end], label=\"predicted\", xlabel=\"day\", ylabel=\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-gender",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
